{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.08068645  1.6377625   1.8093114   3.5947353  21.38956553  6.72244562\n",
      "  26.03046111]]\n"
     ]
    }
   ],
   "source": [
    "def normalize_feats(train_features, some_features):\n",
    "    \"\"\"\n",
    "    Normalizes the sample data features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_features: A numpy array with the shape (N_train, d), where d is the number of features and N_train is the number of training samples.\n",
    "    some_features: A numpy array with the shape (N_some, d), where d is the number of features and N_some is the number of samples to be normalized.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    some_features_normalized: A numpy array with shape (N_some, d).\n",
    "    \"\"\"\n",
    "\n",
    "    # your code here\n",
    "    mean_train = train_features.mean(axis=0)\n",
    "    std_train = train_features.std(axis=0)\n",
    "    some_features_normalized = (some_features - mean_train) / std_train\n",
    "\n",
    "    return some_features_normalized\n",
    "\n",
    "\n",
    "X_train = (np.arange(35).reshape(5,7) ** 13) % 20\n",
    "X_some = np.arange(7).reshape(1,7) * 10\n",
    "X_norm_some = normalize_feats(X_train, X_some)\n",
    "print(X_norm_some)\n",
    "assert np.array_equal(X_norm_some.round(3), np.array([[-1.081, 1.638, 1.809, 3.595, 21.39, 6.722, 26.03 ]]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.98571429]\n",
      " [ 7.04285714]\n",
      " [ 7.52857143]\n",
      " [-4.01428571]\n",
      " [-1.7       ]]\n"
     ]
    }
   ],
   "source": [
    "def e_term(x_batch, y_batch, a, b):\n",
    "    \"\"\"\n",
    "    Computes the margin of the data points.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_batch: A numpy array with the shape (N, d), where d is the number of features and N is the batch size.\n",
    "    y_batch: A numpy array with the shape (N, 1), where N is the batch size.\n",
    "    a: A numpy array with the shape (d, 1), where d is the number of features. This is the weight vector.\n",
    "    b: A scalar.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    e_batch: A numpy array with shape (N, 1).\n",
    "    \"\"\"\n",
    "\n",
    "    # your code here\n",
    "    e_batch = 1 - y_batch * (x_batch @ a + b)\n",
    "\n",
    "    return e_batch\n",
    "\n",
    "x_batch_ = ((np.arange(35, dtype='int64').reshape(5,7) ** 13) % 20) / 7.\n",
    "x = np.arange(35).reshape(5,7) ** 13\n",
    "y_batch_ = (2. * (np.arange(5)>2) - 1.).reshape(-1,1)\n",
    "a_ = (np.arange(7)* 0.2).reshape(-1,1)\n",
    "b_ = 0.1\n",
    "e_batch_ = e_term(x_batch_, y_batch_, a_, b_)\n",
    "print(e_batch_)\n",
    "\n",
    "assert np.array_equal(e_batch_.round(3), np.array([[ 5.986],[ 7.043],[ 7.529],[-4.014],[-1.7  ]]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.67536626785716 5.101213489707721\n"
     ]
    }
   ],
   "source": [
    "def loss_terms_ridge(e_batch, a, lam):\n",
    "    \"\"\"\n",
    "    Computes the hinge and ridge regularization losses.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    e_batch: A numpy array with the shape (N, 1), where N is the batch size. This is the output of the e_term function you wrote previously, and its kth element is e_k = 1 âˆ’ y_k(a*x_k+b).\n",
    "    a: A numpy array with the shape (d, 1), where d is the number of features. This is the weight vector.\n",
    "    lam: A scalar representing the regularization coefficient ðœ†.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    hinge_loss: The hinge regularization loss defined in the above cell.\n",
    "    ridge_loss: The ridge regularization loss defined in the above cell.\n",
    "    \"\"\"\n",
    "\n",
    "    # your code here\n",
    "    #e_batch_ = e_batch.reshape(-1, 1)\n",
    "    N = e_batch.size\n",
    "    hinge_loss = e_batch[e_batch > 0].sum() / N\n",
    "    #ridge_loss = (a.T @ a)[0][0] * lam / 2\n",
    "    ridge_loss = (a**2).sum() * lam / 2\n",
    "\n",
    "    return np.array((hinge_loss, ridge_loss))\n",
    "    #return 1\n",
    "\n",
    "e_batch_ = ((np.arange(35, dtype='int64').reshape(-1,1) ** 13) % 20) / 7.\n",
    "a_ = (np.arange(7)* 0.2).reshape(-1,1)\n",
    "lam_ = 10.\n",
    "\n",
    "hinge_loss_1, reg_loss_1 = tuple(loss_terms_ridge(e_batch_, a_, lam_))\n",
    "assert np.round(hinge_loss_1,3) == 1.114 and np.round(reg_loss_1,3) == 18.2\n",
    "\n",
    "hinge_loss_2, reg_loss_2 = tuple(loss_terms_ridge(e_batch_-1., a_, lam_))\n",
    "assert np.round(hinge_loss_2,3) == 0.412 and np.round(reg_loss_2,3) == 18.2\n",
    "\n",
    "a_1=np.array([[-0.84862344],\n",
    "       [ 0.1468467 ],\n",
    "       [ 0.59857371],\n",
    "       [ 0.28044845],\n",
    "       [-0.39028563],\n",
    "       [ 0.09438289],\n",
    "       [-0.06230917],\n",
    "       [ 0.19013069]])\n",
    "e_batch_1=np.array([[  78.69078901,   11.54573106, -147.75362779, -159.58174745,\n",
    "        -154.77509551, -187.45958187,  126.09360714, -118.50997555],\n",
    "       [  72.87386923,  249.41084942,  226.50586665,  -62.46264488,\n",
    "         293.53719711,  259.44748645,  256.83737789,  158.72302443],\n",
    "       [ 231.48074986,  115.94234727,  256.60601673,   21.12917349,\n",
    "        -174.29941841,  -62.70852331, -195.02513899, -163.03887267],\n",
    "       [ 268.2489063 , -186.96698792, -179.1372718 ,  229.16104564,\n",
    "        -148.37690665, -171.63841744, -109.62947691,  196.52564384],\n",
    "       [ 183.22640848,  -41.12448044, -104.36992412, -141.74909545,\n",
    "         -45.33461071,  292.89233735,  172.36608688,   47.71746109],\n",
    "       [ 165.96268017,  248.28000276, -165.22200816,  292.65417449,\n",
    "        -191.53259981,    7.32711771,    7.87010528,   26.30780551],\n",
    "       [ 235.63002344,   60.90012659, -196.92631629,  -80.84847228,\n",
    "        -174.94622075,  -66.7009531 ,   -0.31161369,    0.76540232],\n",
    "       [-178.95460674,   68.15733965,  -59.42549974,  297.16120174,\n",
    "        -166.81883028,  200.27044571,  251.82169371,  -47.62192147],\n",
    "       [  48.92229377,  263.01703237, -163.52243429,  172.92660644,\n",
    "        -166.37022705,  241.10065631, -185.67946236,  -78.9472858 ],\n",
    "       [ -73.45896188,  199.32790555,  161.7626694 ,  193.86516518,\n",
    "         173.74507505,  284.47381358,  147.51900352,  256.08136337],\n",
    "       [ -64.98602759, -124.46846695, -163.39067543,   70.50457939,\n",
    "         283.0198504 ,   16.58967044,   31.75113891, -189.86563728],\n",
    "       [ 210.90682666,  -85.88673686,  293.61977045,  -36.16159631,\n",
    "          59.12057467,   51.90213059,   -1.069974  ,  189.14035868],\n",
    "       [ 130.51098329,    0.40754881,  -98.84234826,   97.32848935,\n",
    "         136.20810796,  139.03284829, -150.84256303, -133.68335755],\n",
    "       [ 279.14315026,  -18.71370934,  144.8956974 ,  231.9627674 ,\n",
    "         247.9520764 ,  107.11673333,   98.17318232,  141.54285806]])\n",
    "lam_1=np.array(7.39343468)\n",
    "hinge_loss_1, reg_loss_1 = tuple(loss_terms_ridge(e_batch_1, a_1, lam_1))\n",
    "\n",
    "print(hinge_loss_1, reg_loss_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.31428571]\n",
      " [ 2.68571429]\n",
      " [ 5.05714286]\n",
      " [ 6.57142857]\n",
      " [ 8.65714286]\n",
      " [11.02857143]\n",
      " [12.82857143]]\n"
     ]
    }
   ],
   "source": [
    "def a_gradient_ridge(x_batch, y_batch, e_batch, a, lam):\n",
    "    \"\"\"\n",
    "    Computes the ridge_regularized loss gradient w.r.t the weights vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_batch: A numpy array with the shape (N, d), where d is the number of features and N is the batch size.\n",
    "    y_batch: A numpy array with the shape (N, 1), where N is the batch size.\n",
    "    e_batch: A numpy array with the shape (N, 1), where N is the batch size. This is the output of the e_term function you wrote previously, and its kth element is e_k = 1 âˆ’ y_k(a*x_k+b).\n",
    "    a: A numpy array with the shape (d, 1), where d is the number of features. This is the weight vector.\n",
    "    lam: A scalar representing the regularization coefficient ðœ†.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grad_a: A numpy array with shape (d, 1) and defined as the gradient of the ridge regularized loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    # your code here\n",
    "    N = x_batch.shape[0]\n",
    "    g = np.sum(-1 * y_batch * x_batch * (e_batch > 0), axis=0, keepdims=True) / N\n",
    "    grad_a = (lam * a) + g.T\n",
    "\n",
    "    return grad_a\n",
    "\n",
    "\n",
    "# Performing sanity checks on your implementation\n",
    "x_batch_ = ((np.arange(35, dtype='int64').reshape(5,7) ** 13) % 20) / 7.\n",
    "y_batch_ = (2. * (np.arange(5)>2) - 1.).reshape(-1,1)\n",
    "a_ = (np.arange(7)* 0.2).reshape(-1,1)\n",
    "b_ = 0.1\n",
    "lam_ = 10.\n",
    "e_batch_ = e_term(x_batch_, y_batch_, a_, b_)\n",
    "\n",
    "grad_a_ = a_gradient_ridge(x_batch_, y_batch_, e_batch_, a_, lam_)\n",
    "print(grad_a_)\n",
    "\n",
    "assert np.array_equal(grad_a_.round(3), np.array([[ 0.314],[ 2.686],[ 5.057],[ 6.571],[ 8.657],[11.029],[12.829]]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "def b_derivative(y_batch, e_batch):\n",
    "    \"\"\"\n",
    "    Computes the loss gradient with respect to the bias parameter b.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_batch: A numpy array with the shape (N, 1), where N is the batch size.\n",
    "    e_batch: A numpy array with the shape (N, 1), where N is the batch size. This is the output of the e_term function you wrote previously, and its kth element is e_k = 1 âˆ’ y_k(a*x_k+b).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    der_b: A scalar defined as the gradient of the hinge loss w.r.t the bias parameter b.\n",
    "    \"\"\"\n",
    "\n",
    "    # your code here\n",
    "    N = y_batch.shape[0]\n",
    "    der_b = np.sum(-1 * y_batch * (e_batch > 0)) / N\n",
    "\n",
    "    return der_b\n",
    "\n",
    "# Performing sanity checks on your implementation\n",
    "x_batch_ = ((np.arange(35, dtype='int64').reshape(5,7) ** 13) % 20) / 7.\n",
    "y_batch_ = (2. * (np.arange(5)>2) - 1.).reshape(-1,1)\n",
    "a_ = (np.arange(7)* 0.2).reshape(-1,1)\n",
    "b_ = -5.\n",
    "e_batch_ = e_term(x_batch_, y_batch_, a_, b_)\n",
    "\n",
    "grad_b_ = b_derivative(y_batch_, e_batch_)\n",
    "print(grad_b_)\n",
    "\n",
    "assert np.round(grad_b_, 3) == 0.2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "def loss_terms_lasso(e_batch, a, lam):\n",
    "    \"\"\"\n",
    "    Computes the hinge and lasso regularization losses.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    e_batch: A numpy array with the shape (N, 1), where N is the batch size. This is the output of the e_term function you wrote previously, and its kth element is e_k = 1 âˆ’ y_k(a*x_k+b).\n",
    "    a: A numpy array with the shape (d, 1), where d is the number of features. This is the weight vector.\n",
    "    lam: A scalar representing the regularization coefficient ðœ†.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    hinge_loss: The hinge loss scalar as defined in the cell above.\n",
    "    lasso_loss: The lasso loss scalar as defined in the cell above.\n",
    "    \"\"\"\n",
    "\n",
    "    # your code here\n",
    "    N = e_batch.size\n",
    "    hinge_loss = e_batch[e_batch > 0].sum() / N\n",
    "    lasso_loss = lam * np.abs(a).sum()\n",
    "\n",
    "    return np.array((hinge_loss, lasso_loss))\n",
    "\n",
    "# Performing sanity checks on your implementation\n",
    "e_batch_ = ((np.arange(35, dtype='int64').reshape(-1,1) ** 13) % 20) / 7.\n",
    "a_ = (np.arange(7)* 0.2).reshape(-1,1)\n",
    "lam_ = 10.\n",
    "\n",
    "hinge_loss_1, reg_loss_1 = tuple(loss_terms_lasso(e_batch_, a_, lam_))\n",
    "assert np.round(hinge_loss_1,3) == 1.114 and np.round(reg_loss_1,3) == 42.0, np.round(reg_loss_1,3)\n",
    "\n",
    "hinge_loss_2, reg_loss_2 = tuple(loss_terms_lasso(e_batch_-1., a_, lam_))\n",
    "assert np.round(hinge_loss_2,3) == 0.412 and np.round(reg_loss_2,3) == 42.0, np.round(reg_loss_2,3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "def a_gradient_lasso(x_batch, y_batch, e_batch, a, lam):\n",
    "    \"\"\"\n",
    "    Computes the lasso-regularized loss sub-gradient w.r.t the weights vector\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_batch: A numpy array with the shape (N, d), where d is the number of features and N is the batch size.\n",
    "    y_batch: A numpy array with the shape (N, 1), where N is the batch size.\n",
    "    e_batch: A numpy array with the shape (N, 1), where N is the batch size. This is the output of the e_term function you wrote previously, and its kth element is e_k = 1 âˆ’ y_k(a*x_k+b).\n",
    "    a: A numpy array with the shape (d, 1), where d is the number of features. This is the weight vector.\n",
    "    lam: A scalar representing the regularization coefficient ðœ†.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grad_a: A numpy array with shape (d, 1) and defined as the gradient of the lasso-regularized loss function w.r.t the weights vector.\n",
    "    \"\"\"\n",
    "\n",
    "    # your code here\n",
    "    N = x_batch.shape[0]\n",
    "    g = np.sum(-1 * y_batch * x_batch * (e_batch > 0), axis=0, keepdims=True).T / N\n",
    "    sign_a = (a > 0) + ((a < 0) * -1 )\n",
    "    grad_a = (lam * sign_a) + g\n",
    "\n",
    "    return grad_a\n",
    "\n",
    "# Performing sanity checks on your implementation\n",
    "x_batch_ = ((np.arange(35, dtype='int64').reshape(5,7) ** 13) % 20) / 7.\n",
    "y_batch_ = (2. * (np.arange(5)>2) - 1.).reshape(-1,1)\n",
    "a_ = (np.arange(7)* 0.2).reshape(-1,1)\n",
    "b_ = 0.1\n",
    "lam_ = 10.\n",
    "e_batch_ = e_term(x_batch_, y_batch_, a_, b_)\n",
    "\n",
    "grad_a_lasso_ = a_gradient_lasso(x_batch_, y_batch_, e_batch_, a_, lam_)\n",
    "\n",
    "assert np.array_equal(grad_a_lasso_.round(3), np.array([[ 0.314], [10.686], [11.057],[10.571], [10.657], [11.029], [10.829]]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}